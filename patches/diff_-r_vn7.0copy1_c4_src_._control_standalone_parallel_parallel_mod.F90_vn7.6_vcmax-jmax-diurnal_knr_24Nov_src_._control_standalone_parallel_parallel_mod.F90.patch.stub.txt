Patch stub generated: 2025-11-25T12:09:12.151599 UTC

HEADER:
diff -r vn7.0copy1_c4/src/./control/standalone/parallel/parallel_mod.F90 vn7.6_vcmax-jmax-diurnal_knr_24Nov/src/./control/standalone/parallel/parallel_mod.F90

SUMMARY:
  - added lines (>): 517
  - removed lines (<): 12
  - only in one tree: False

RECOMMENDATIONS:
- Manual review recommended; inspect the preview below.

DIFF PREVIEW:
diff -r vn7.0copy1_c4/src/./control/standalone/parallel/parallel_mod.F90 vn7.6_vcmax-jmax-diurnal_knr_24Nov/src/./control/standalone/parallel/parallel_mod.F90
8,14d7
< 
< MODULE parallel_mod
< 
< USE logging_mod, ONLY: log_info, log_fatal
< 
< IMPLICIT NONE
< 
26a20,25
> MODULE parallel_mod
> 
> USE logging_mod, ONLY: log_info, log_fatal
> 
> IMPLICIT NONE
> 
56d54
< 
58a57,566
> !-----------------------------------------------------------------------------
> ! Description:
> !   Returns .TRUE. if the current task is the master task, .FALSE. otherwise
> !-----------------------------------------------------------------------------
> LOGICAL FUNCTION is_master_task()
> 
> USE jules_vars_mod, ONLY: mpi_local_comm
> 
> IMPLICIT NONE
> 
> ! Work variables
> INTEGER :: task_id  ! The id of this task
> 
> INTEGER :: ERROR  ! Error indicator for MPI calls
>                   ! This is ignored as (most) MPI implementations fail
>                   ! rather than returning actual error codes
> 
> 
> !-----------------------------------------------------------------------------
> 
> CALL mpi_comm_rank(mpi_local_comm, task_id, ERROR)
> 
> is_master_task = (task_id == master_task_id)
> 
> RETURN
> 
> END FUNCTION is_master_task
> 
> !-----------------------------------------------------------------------------
> ! Description:
> !   Decomposes the given grid across the available MPI tasks
> !   Returns a subgrid object representing the part of the grid that the
> !   current task will be responsible for
> !-----------------------------------------------------------------------------
> FUNCTION decompose_domain(grid) RESULT(task_subgrid)
> 
> USE mpi, ONLY: mpi_address_kind, mpi_real
> USE jules_vars_mod, ONLY: mpi_local_comm
> 
> USE grid_utils_mod, ONLY: grid_info, subgrid_info, subgrid_create
> 
> USE string_utils_mod, ONLY: to_string
> 
> IMPLICIT NONE
> 
> ! Argument types
> TYPE(grid_info), INTENT(IN) :: grid  ! The grid to decompose
> 
> ! Return type
> TYPE(subgrid_info) :: task_subgrid  ! The subgrid this task is responsible for
> 
> ! Work variables
> INTEGER :: ntasks_x, ntasks_y  ! The size of the "task grid" (i.e. the grid
>                                ! will be split into ntasks_x by ntasks_y
>                                ! blocks
> 
> INTEGER :: task_nx, task_ny    ! The size of the block for the current task
> 
> INTEGER :: task_x, task_y  ! The x and y coordinates in the "task grid"
>                            ! of the current task
> 
> INTEGER :: x_start, y_start  ! The x/y coordinates in the grid of the start
>                              ! of the task subgrid
> 
> LOGICAL :: found_decomposition  ! T - we found a usable decomposition
>                                 ! F - we did not find a usable decomposition
> 
> INTEGER :: leftover  ! The remainder when distributing columns along a task row
> 
> INTEGER :: mpi_type  ! Holds intermediate MPI datatype before the extent
>                      ! is adjusted
> INTEGER(KIND=mpi_address_kind) :: mpi_real_lb, mpi_real_extent
>                      ! The lower bound and extent for the mpi_real type
> 
> INTEGER(KIND=mpi_address_kind), PARAMETER :: mpi_zero = 0
>                      ! A 'zero' of the correct kind to be used as an MPI address
> 
> INTEGER, ALLOCATABLE :: counts_2d(:,:), offsets_2d(:,:)
>                      ! Used when calculating the counts and offsets on the
>                      ! task grid
> 
> INTEGER :: ERROR  ! Error indicator for MPI calls
>                   ! This is ignored as (most) MPI implementations fail
>                   ! rather than returning actual error codes
> 
> INTEGER :: i, j, n  ! Index variables
> 
> 
> !-----------------------------------------------------------------------------
> 
> 
> !-----------------------------------------------------------------------------
> ! This routine currently implements a very naive decomposition
> !
> ! The main concern when decomposing the grid is I/O, not MPI communication
> ! I.E. the grid must be split into contiguous regions that can be written to
> ! file in one write statement by specifying appropriate start and count
> !-----------------------------------------------------------------------------
> 
> ! First get the number of available tasks and the id of this task
> CALL mpi_comm_size(mpi_local_comm, ntasks, ERROR)
> CALL mpi_comm_rank(mpi_local_comm, task_id, ERROR)
> 
> CALL log_info("decompose_domain",                                              &
>               "Decomposing domain across " // TRIM(to_string(ntasks)) //       &
>               " available tasks")
> 
> ! We can only utilise at most 1 task per point
> IF ( ntasks > grid%nx * grid%ny )                                              &
>   CALL log_fatal("decompose_domain",                                           &
>                  "More tasks are available than points in the model grid")
> 
> !-----------------------------------------------------------------------------
> ! Work out the decomposition, subject to the following rules:
> !
> !   * Each task must have the same number of grid rows, but can have a
> !     varying number of columns
> !
> !   * Each row of the task grid must have at most grid%nx tasks
> !
> !   * Each row of the task grid must have the same number of tasks
> !
> !-----------------------------------------------------------------------------
> ! This is the minimum number of rows we need in the task grid to ensure that
> ! each row has <= grid%nx tasks
> ntasks_y = (ntasks-1) / grid%nx + 1
> 
> ! Loop until we find a suitable number of rows for the task grid
> ! Limited testing found that using as many rows of tasks as possible resulted
> ! in the most efficient decomposition more of the time (I realise that sounds
> ! a bit woolly!)
> DO n = grid%ny,ntasks_y,-1
>   found_decomposition =     ( MOD(grid%ny, n) == 0 ) & ! Each task gets the same number of grid rows
>                       .AND. ( MOD(ntasks, n) == 0 )     ! Each row of the task grid has the same number of tasks
> 
>   IF ( found_decomposition ) THEN
>     ntasks_x = ntasks / n
>     ntasks_y = n
>     EXIT
>   END IF
> END DO

INSTRUCTIONS:
  1) Edit the corresponding .patch file: patches/diff_-r_vn7.0copy1_c4_src_._control_standalone_parallel_parallel_mod.F90_vn7.6_vcmax-jmax-diurnal_knr_24Nov_src_._control_standalone_parallel_parallel_mod.F90.patch
     - Place a git-style unified diff (``--- a/..., +++ b/...``) there.
  2) Create small focused patches (one logical change per file).
  3) Run apply_patches.sh (after filling patches) from the repository root.

