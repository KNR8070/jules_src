###############################################################################
## Common classes
###############################################################################
    [[METO_EX1A]]
        [[[job]]]
            batch system = pbs
            execution time limit = PT10M
        [[[remote]]]
            host = login.exz
        [[[directives]]]
            -S = /bin/bash

    [[METO_EX1A_CCE_ENV]]
        env-script = """
                     module switch PrgEnv-cray PrgEnv-cray/8.3.3
                     module load cpe/22.05
                     module load cray-hdf5-parallel/1.12.1.1
                     module load cray-netcdf-hdf5parallel/4.8.1.1
                     module load nccmp/1.9.1.0
                     module list 2>&1
                     """

    [[METO_EX1A_GFORTRAN_ENV]]
        env-script = """
                     module switch PrgEnv-cray PrgEnv-gnu/8.3.3
                     module load cpe/22.05
                     module load cray-hdf5-parallel/1.12.1.1
                     module load cray-netcdf-hdf5parallel/4.8.1.1
                     module load nccmp/1.9.1.0
                     module list 2>&1
                     """
 
# The queue family for jobs that will not use a whole compute node. For the foreseeable future
# jules jobs are expected not to take up more than one whole node (256 cores). Note using this 
# family requires that the environment variable MPI_NUM_TASKS be set to the number of MPI tasks,
# and OMP_NUM_THREADS set to number of omp threads per task. 
    [[METO_EX1A_QUEUE]]
        inherit = None, METO_EX1A
        [[[directives]]]
            -q = shared
        [[[environment]]]
            ROSE_LAUNCHER = mpiexec
            ROSE_LAUNCHER_PREOPTS = --cpu-bind=depth -n ${MPI_NUM_TASKS:-1} -d $OMP_NUM_THREADS
            ROSE_LAUNCHER_ULIMIT_OPTS = -s unlimited -c unlimited

# Families for different counts of CPUs, nodes, MPI tasks and OpenMP threads
# We need all these separate families to make sure environment variables come out in the correct order
# Each compute job should inherit from one each of the CORES, OMPTHREADS and MPITASKS groups, in that order

# Note that each of the METO_EX1A_CORES_N families set -l ncpus = 2N. This is to avoid accidentally
# using hyperthreads/SMT incorrectly.
    [[METO_EX1A_CORES_1]]
        inherit = METO_EX1A_QUEUE
        [[[directives]]]
            -l ncpus = 2
            -l mem = 2G

# 8 physical cores == 1 Milan chiplet
    [[METO_EX1A_CORES_8]]
        inherit = METO_EX1A_QUEUE
        [[[directives]]]
            -l ncpus = 16
            -l mem = 16G

# 32 physical cores == 4 Milan chiplets
    [[METO_EX1A_CORES_32]]
        inherit = METO_EX1A_QUEUE
        [[[directives]]]
            -l ncpus = 64
            -l mem = 64G

    [[METO_EX1A_OMPTHREADS_1]]
        [[[environment]]]
            OMP_NUM_THREADS = 1

    [[METO_EX1A_OMPTHREADS_8]]
        [[[environment]]]
            OMP_NUM_THREADS = 8

    [[METO_EX1A_MPITASKS_1]]
        [[[environment]]]
            MPI_NUM_TASKS = 1
            NPROC = $MPI_NUM_TASKS
    
    [[METO_EX1A_MPITASKS_4]]
        [[[environment]]]
            MPI_NUM_TASKS = 4
            NPROC = $MPI_NUM_TASKS

    [[METO_EX1A_MPITASKS_8]]
        [[[environment]]]
            MPI_NUM_TASKS = 8
            NPROC = $MPI_NUM_TASKS

    [[METO_EX1A_MPITASKS_32]]
        [[[environment]]]
            MPI_NUM_TASKS = 32
            NPROC = $MPI_NUM_TASKS


